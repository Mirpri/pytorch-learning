{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Seungjae Lee (이승재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $H(x)$: How to make predictions for a given $x$ value\n",
    " - $cost(W, b)$: How well $H(x)$ predicts $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xff22f0737030>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use fake data for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 PyTorch는 NCHW 형태이다.\n",
    "(Basically, PyTorch is in NCHW form.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost means \"loss\", it's a function that measures how well the neural network's predictions match the true data. During training, the goal is to minimize this loss function.\n",
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.],\n",
      "        [-2.],\n",
      "        [-3.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6667, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient descent is a fundamental optimization technique commonly used in machine learning and deep learning to minimize the loss function and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01) # defines an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "`optim.SGD`:\n",
    "\n",
    "`optim` is a module in PyTorch that contains various optimization algorithms.\n",
    "`SGD` stands for Stochastic Gradient Descent, which is a popular optimization algorithm used for training machine learning models.\n",
    "\n",
    "`[W, b]`:\n",
    "\n",
    "This is a list of parameters that the optimizer will update. In this case, `W` and `b` are the parameters of the model (e.g., weights and biases) that require gradient updates.\n",
    "These parameters should be tensors with `requires_grad=True` so that gradients can be computed for them during backpropagation.\n",
    "\n",
    "lr=0.01:\n",
    "\n",
    "lr stands for learning rate, which is a hyperparameter that controls the step size during the parameter updates.\n",
    "A smaller learning rate means smaller updates and can lead to more precise convergence, but it might take longer to train.\n",
    "A larger learning rate means larger updates and can speed up training, but it might overshoot the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # initialize gradient\n",
    "cost.backward() # compute gradient\n",
    "optimizer.step() # update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`optimizer.zero_grad()`**:\n",
    "   - This function sets the gradients of all model parameters to zero. \n",
    "   - Gradients are accumulated by default in PyTorch, so if you don't zero them out, the gradients from the previous iteration will be added to the current gradients, which is not what you want.\n",
    "   - This step ensures that you start with a clean slate for the current iteration.\n",
    "\n",
    "2. **`cost.backward()`**:\n",
    "   - This function computes the gradient of the loss (cost) with respect to each parameter (i.e., `W` and `b`) using backpropagation.\n",
    "   - The gradients are stored in the `.grad` attribute of each parameter tensor.\n",
    "   - These gradients indicate the direction and magnitude of change needed to reduce the loss.\n",
    "\n",
    "3. **`optimizer.step()`**:\n",
    "   - This function updates the parameters using the gradients computed in the previous step.\n",
    "   - The optimizer adjusts the parameters (`W` and `b`) based on the gradients and the learning rate.\n",
    "   - For example, in Stochastic Gradient Descent (SGD), the update rule is:\n",
    "     ```python\n",
    "     W = W - learning_rate * W.grad\n",
    "     b = b - learning_rate * b.grad\n",
    "     ```\n",
    "   - This step moves the parameters in the direction that reduces the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0933], requires_grad=True)\n",
      "tensor([0.0400], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `W` and `b` Change:\n",
    "\n",
    "- **Gradient Computation**: During `cost.backward()`, the gradients of the loss with respect to `W` and `b` are computed. These gradients tell us how much and in which direction to change `W` and `b` to reduce the loss.\n",
    "- **Parameter Update**: During `optimizer.step()`, the optimizer uses these gradients to update `W` and `b`. The parameters are adjusted slightly in the direction that reduces the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the hypothesis is now better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1333],\n",
      "        [0.2267],\n",
      "        [0.3200]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6927, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating through this process, the model parameters are optimized to fit the data better, reducing the loss over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we will be training on the dataset for multiple epochs. This can be done simply with loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.360, b: 0.160 Cost: 70.000000\n",
      "Epoch  100/2000 W: 3.197, b: 1.553 Cost: 0.028801\n",
      "Epoch  200/2000 W: 3.155, b: 1.649 Cost: 0.017797\n",
      "Epoch  300/2000 W: 3.122, b: 1.724 Cost: 0.010998\n",
      "Epoch  400/2000 W: 3.096, b: 1.783 Cost: 0.006796\n",
      "Epoch  500/2000 W: 3.075, b: 1.829 Cost: 0.004199\n",
      "Epoch  600/2000 W: 3.059, b: 1.866 Cost: 0.002595\n",
      "Epoch  700/2000 W: 3.046, b: 1.895 Cost: 0.001604\n",
      "Epoch  800/2000 W: 3.036, b: 1.917 Cost: 0.000991\n",
      "Epoch  900/2000 W: 3.029, b: 1.935 Cost: 0.000612\n",
      "Epoch 1000/2000 W: 3.023, b: 1.949 Cost: 0.000378\n",
      "Epoch 1100/2000 W: 3.018, b: 1.960 Cost: 0.000234\n",
      "Epoch 1200/2000 W: 3.014, b: 1.968 Cost: 0.000144\n",
      "Epoch 1300/2000 W: 3.011, b: 1.975 Cost: 0.000089\n",
      "Epoch 1400/2000 W: 3.009, b: 1.980 Cost: 0.000055\n",
      "Epoch 1500/2000 W: 3.007, b: 1.985 Cost: 0.000034\n",
      "Epoch 1600/2000 W: 3.005, b: 1.988 Cost: 0.000021\n",
      "Epoch 1700/2000 W: 3.004, b: 1.990 Cost: 0.000013\n",
      "Epoch 1800/2000 W: 3.003, b: 1.993 Cost: 0.000008\n",
      "Epoch 1900/2000 W: 3.003, b: 1.994 Cost: 0.000005\n",
      "Epoch 2000/2000 W: 3.002, b: 1.995 Cost: 0.000003\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[5], [8], [11]])\n",
    "# Initialize model\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer settings\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) calculate\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost calculate\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # improve H(x) using cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Output log every 100 times\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# Initialize model\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer settings\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) calculate\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost calculate\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost and H(x) calculate\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Output log every 100 times\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will rewrite the program using linear regression model in `torch.nn`.\n",
    "\n",
    "Remember that we had this fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a linear regression model. By default, all models in PyTorch are created by inheriting from the provided `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Definition**\n",
    "\n",
    "LinearRegressionModel is a subclass of `nn.Module`, which is the base class for all neural network modules in PyTorch.\n",
    "\n",
    "By inheriting from `nn.Module`, this class gains access to a variety of useful methods and properties for building and training neural networks.\n",
    "\n",
    "**Constructor Method**\n",
    "- The `__init__` method is the constructor of the class. It initializes the instance of the class.\n",
    "\n",
    "-  `super()` is a built-in function that allows you to call methods from a parent class. `super().__init__()` calls the constructor of the parent class (`nn.Module`). This is necessary to properly initialize the base class.\n",
    "\n",
    "- `self.linear = nn.Linear(1, 1)` creates a linear layer with one input feature and one output feature. This layer will perform the linear transformation \\( y = xW + b \\), where \\( W \\) is the weight and \\( b \\) is the bias.\n",
    "\n",
    "**Forward Method**\n",
    "- The `forward` method defines the forward pass of the model. This is where the input data is passed through the network.\n",
    "- `x` is the input tensor.\n",
    "- `return self.linear(x)` applies the linear transformation defined in the `self.linear` layer to the input `x` and returns the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `__init__` method of the model, we define the layers to be used. Here, we will use `nn.Linear` because we are creating a linear regression model. In the `forward` method, we specify how the model calculates the output from the input.\n",
    "\n",
    "nn.Linear(1,1) specifies the input and output dimentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a model and calculate the predicted value $H(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2248],\n",
      "        [-0.8860],\n",
      "        [-1.5473]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the cost using mean squared error (MSE). MSE is also provided by default in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2248],\n",
      "        [-0.8860],\n",
      "        [-1.5473]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.1690, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the final given cost, we adjust $H(x)$'s $W$ and $b$ to reduce the cost. At this time, one of the `optimizers` in PyTorch's `torch.optim` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the Linear Regression code, let's run the code to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.224, b: -0.086 Cost: 3.896044\n",
      "Epoch  100/1000 W: 0.926, b: 0.168 Cost: 0.004088\n",
      "Epoch  200/1000 W: 0.942, b: 0.132 Cost: 0.002526\n",
      "Epoch  300/1000 W: 0.954, b: 0.104 Cost: 0.001561\n",
      "Epoch  400/1000 W: 0.964, b: 0.082 Cost: 0.000965\n",
      "Epoch  500/1000 W: 0.972, b: 0.064 Cost: 0.000596\n",
      "Epoch  600/1000 W: 0.978, b: 0.051 Cost: 0.000368\n",
      "Epoch  700/1000 W: 0.983, b: 0.040 Cost: 0.000228\n",
      "Epoch  800/1000 W: 0.986, b: 0.031 Cost: 0.000141\n",
      "Epoch  900/1000 W: 0.989, b: 0.025 Cost: 0.000087\n",
      "Epoch 1000/1000 W: 0.992, b: 0.019 Cost: 0.000054\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# initialize model\n",
    "model = LinearRegressionModel()\n",
    "# optimizer setting\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) calculate\n",
    "    prediction = model(x_train) #this is same as model.forward(x_train), where the forward method is called\n",
    "    \n",
    "    # cost calculate\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost and H(x) improvement \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # log every 100 times\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `cost` is decending as `W` and `b` is adjusted.\n",
    "\n",
    "Compared with the previous version, this code utilizes the linear regression model in torch to make the code easier and reusable. Specifically, model initialization, H(x) calculation and ways to access W and b are changed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
