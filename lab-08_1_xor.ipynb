{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8_1: Implementing XOR with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xff5fad1cecf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for reproducibility\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data: XOR situations\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear = torch.nn.Linear(2, 1, bias=True) # input_dim = 2, output_dim = 1\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` is a container module in PyTorch that allows you to build neural networks by stacking layers in a sequential manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Sequential(linear, sigmoid).to(device) # first linear, then sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy (BCE) Loss, also known as Log Loss, is a loss function commonly used in binary classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "Here's the formula for Binary Cross-Entropy Loss:\n",
    "\n",
    "$ \\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)] $\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the number of samples.\n",
    "- \\( y_i \\) is the actual label (0 or 1) for the \\( i \\)-th sample.\n",
    "- \\( p_i \\) is the predicted probability for the \\( i \\)-th sample.\n",
    "\n",
    "### Explanation:\n",
    "- The BCE loss function penalizes the model more when it makes confident but incorrect predictions.\n",
    "- If the actual label \\( y_i \\) is 1 and the predicted probability \\( p_i \\) is close to 0, the loss will be very high.\n",
    "- Conversely, if \\( y_i \\) is 0 and \\( p_i \\) is close to 1, the loss will also be very high.\n",
    "- The goal is to minimize this loss function during training, which means the model's predicted probabilities should be as close as possible to the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7273973822593689\n",
      "100 0.6951808929443359\n",
      "200 0.693813681602478\n",
      "300 0.6933773756027222\n",
      "400 0.6932311654090881\n",
      "500 0.6931794881820679\n",
      "600 0.6931601762771606\n",
      "700 0.6931526064872742\n",
      "800 0.6931495070457458\n",
      "900 0.6931481957435608\n",
      "1000 0.693147599697113\n",
      "1100 0.6931474208831787\n",
      "1200 0.6931473016738892\n",
      "1300 0.6931472420692444\n",
      "1400 0.6931471824645996\n",
      "1500 0.6931471824645996\n",
      "1600 0.6931471824645996\n",
      "1700 0.6931471824645996\n",
      "1800 0.6931471824645996\n",
      "1900 0.6931471228599548\n",
      "0.6931471824645996\n",
      "0.weight tensor([[-5.1122e-05, -4.9037e-05]])\n",
      "0.bias tensor([5.9384e-05])\n"
     ]
    }
   ],
   "source": [
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())\n",
    "print(cost.item())\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[0.50001484]\n",
      " [0.5000026 ]\n",
      " [0.5000021 ]\n",
      " [0.4999898 ]] \n",
      "Correct:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
